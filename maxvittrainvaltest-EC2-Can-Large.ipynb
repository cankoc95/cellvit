{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rKbz-q1dYGVp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.9/site-packages (2.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (4.63.2)\n",
            "Requirement already satisfied: responses<0.19 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (5.4.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (2.28.2)\n",
            "Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2023.3.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 kB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (4.63.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, regex, transformers\n",
            "Successfully installed regex-2023.3.23 tokenizers-0.13.2 transformers-4.27.3\n",
            "Collecting timm\n",
            "  Downloading timm-0.8.17.dev0-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from timm) (0.13.3)\n",
            "Requirement already satisfied: torch>=1.7 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from timm) (1.13.1)\n",
            "Collecting safetensors\n",
            "  Downloading safetensors-0.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from timm) (5.4.1)\n",
            "Requirement already satisfied: torchvision in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from timm) (0.14.1)\n",
            "Requirement already satisfied: typing_extensions in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub->timm) (4.63.2)\n",
            "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub->timm) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub->timm) (21.3)\n",
            "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub->timm) (2.28.2)\n",
            "Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (2.1.1)\n",
            "Installing collected packages: safetensors, timm\n",
            "Successfully installed safetensors-0.3.0 timm-0.8.17.dev0\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from wandb) (5.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from wandb) (2.28.2)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from wandb) (4.5.0)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from wandb) (3.20.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from wandb) (5.9.4)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from wandb) (67.6.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.17.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=14f763b5198c7f118201eee152be83f8b897cc2dda63c1bf39a822c2141b47ab\n",
            "  Stored in directory: /home/ubuntu/.cache/pip/wheels/20/7c/09/4ad42725a29fce4bc21137c7f25f062b3655a4aea5b0e8d9a2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.17.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from opencv-python) (1.23.5)\n",
            "Installing collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.7.0.72\n"
          ]
        }
      ],
      "source": [
        "# !pip install torch torchvision\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install --pre timm\n",
        "!pip install wandb\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tO7DjSN6YDov"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.pyplot._IonContext at 0x7f974842a2e0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import timm\n",
        "import pandas as pd\n",
        "# from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.optim import Adam, AdamW\n",
        "\n",
        "from datasets import Dataset\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "import cv2\n",
        "import wandb\n",
        "import uuid\n",
        "import tempfile\n",
        "from datetime import datetime, date\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bj-zSDkzpl7h"
      },
      "outputs": [],
      "source": [
        "CELL_PAINTING_DIR = '/home/ubuntu/src'\n",
        "MODEL_OUTPUT_DIR = '/dev/shm'\n",
        "CACHE_DIR = CELL_PAINTING_DIR + \"/cache\"\n",
        "BROAD_DIR = CELL_PAINTING_DIR + '/data/cpg0019-moshkov-deepprofiler/broad'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3M7YRoHJIrxK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "class_labels_to_int = {\n",
        "    'AKT1_E17K': 0,\n",
        "    'AKT1_WT': 1,\n",
        "    'BRAF_V600E': 2,\n",
        "    'BRAF_WT': 3,\n",
        "    'CDC42_Q61L': 4,\n",
        "    'CDC42_T17N': 5,\n",
        "    'CDC42_WT': 6,\n",
        "    'EMPTY': 7,\n",
        "    'KRAS_G12V': 8,\n",
        "    'KRAS_WT': 9,\n",
        "    'RAF1_L613V': 10,\n",
        "    'RAF1_WT': 11,\n",
        "    'RHOA_Q63L': 12,\n",
        "    'RHOA_WT': 13\n",
        "}\n",
        "print(len(class_labels_to_int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YVqgh9ATWVc"
      },
      "source": [
        "### Prepare train/dev dataset (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F56q2a60T1ZI"
      },
      "outputs": [],
      "source": [
        "def add_image_paths(row, delim='../../', prefix=BROAD_DIR):\n",
        "    _, path_suffix = row.Image_Name.split(delim)\n",
        "    path = prefix + \"/\" + path_suffix\n",
        "    return path\n",
        "\n",
        "def do_the_thing(dataset_name: str):\n",
        "    from data_fetcher import DataFetcher\n",
        "    dataset = pd.read_parquet(CACHE_DIR + f\"/{dataset_name}_metadata.parquet\", engine=\"pyarrow\")\n",
        "\n",
        "    c037_27k_dataset['Path'] = c037_27k_dataset.apply(lambda x: add_image_paths(x), axis=1)\n",
        "    c037_27k_dataset['Labels'] = c037_27k_dataset[\"Treatment\"].replace(\n",
        "          to_replace=class_labels_to_int)\n",
        "    \n",
        "    c037_27k_dataset_clean = DataFetcher.clean_data(c037_27k_dataset)\n",
        "    c037_27k_dataset_clean.shape\n",
        "\n",
        "    DataFetcher.create_train_dev_split(c037_27k_dataset_clean, CACHE_DIR)\n",
        "\n",
        "    # Add channel information to each train and dev dataset\n",
        "    for dataset_t in [\"train\", \"dev\"]:\n",
        "    dataset_file = CACHE_DIR + f\"/{dataset_t}/data.parquet\"\n",
        "    dataset_df = pd.read_parquet(dataset_file, engine=\"pyarrow\")\n",
        "    dataset_df = DataFetcher.repeat_channels(dataset_df)\n",
        "    dataset_df = dataset_df.drop(columns=['__index_level_0__'])\n",
        "    dataset_df.to_parquet(dataset_file, engine=\"pyarrow\")\n",
        "    print(f\"Added channels to {dataset_file}, shape is: {dataset_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v3kjpBM2HmHb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(27448, 16)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Collection</th>\n",
              "      <th>Metadata_Plate</th>\n",
              "      <th>Metadata_Well</th>\n",
              "      <th>Metadata_Site</th>\n",
              "      <th>Nuclei_Location_Center_X</th>\n",
              "      <th>Nuclei_Location_Center_Y</th>\n",
              "      <th>Image_Name</th>\n",
              "      <th>Treatment</th>\n",
              "      <th>Treatment_Type</th>\n",
              "      <th>Control</th>\n",
              "      <th>Cell_line</th>\n",
              "      <th>LeaveReplicatesOut</th>\n",
              "      <th>LeaveCellsOut</th>\n",
              "      <th>PathId</th>\n",
              "      <th>Path</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8255825</th>\n",
              "      <td>BBBC037</td>\n",
              "      <td>41754</td>\n",
              "      <td>l09</td>\n",
              "      <td>2</td>\n",
              "      <td>776</td>\n",
              "      <td>538</td>\n",
              "      <td>../../training_images/BBBC037/41754/l09/2/105@...</td>\n",
              "      <td>EMPTY</td>\n",
              "      <td>ORF</td>\n",
              "      <td>Control</td>\n",
              "      <td>U2OS</td>\n",
              "      <td>Training</td>\n",
              "      <td>NotUsed</td>\n",
              "      <td>105@776x538.png</td>\n",
              "      <td>/home/ubuntu/src/data/cpg0019-moshkov-deepprof...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8314697</th>\n",
              "      <td>BBBC037</td>\n",
              "      <td>41755</td>\n",
              "      <td>m16</td>\n",
              "      <td>2</td>\n",
              "      <td>522</td>\n",
              "      <td>724</td>\n",
              "      <td>../../training_images/BBBC037/41755/m16/2/105@...</td>\n",
              "      <td>EMPTY</td>\n",
              "      <td>ORF</td>\n",
              "      <td>Control</td>\n",
              "      <td>U2OS</td>\n",
              "      <td>Training</td>\n",
              "      <td>NotUsed</td>\n",
              "      <td>105@522x724.png</td>\n",
              "      <td>/home/ubuntu/src/data/cpg0019-moshkov-deepprof...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8321958</th>\n",
              "      <td>BBBC037</td>\n",
              "      <td>41755</td>\n",
              "      <td>p23</td>\n",
              "      <td>5</td>\n",
              "      <td>953</td>\n",
              "      <td>186</td>\n",
              "      <td>../../training_images/BBBC037/41755/p23/5/105@...</td>\n",
              "      <td>EMPTY</td>\n",
              "      <td>ORF</td>\n",
              "      <td>Control</td>\n",
              "      <td>U2OS</td>\n",
              "      <td>Training</td>\n",
              "      <td>NotUsed</td>\n",
              "      <td>105@953x186.png</td>\n",
              "      <td>/home/ubuntu/src/data/cpg0019-moshkov-deepprof...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8264069</th>\n",
              "      <td>BBBC037</td>\n",
              "      <td>41754</td>\n",
              "      <td>o21</td>\n",
              "      <td>3</td>\n",
              "      <td>525</td>\n",
              "      <td>56</td>\n",
              "      <td>../../training_images/BBBC037/41754/o21/3/105@...</td>\n",
              "      <td>EMPTY</td>\n",
              "      <td>ORF</td>\n",
              "      <td>Control</td>\n",
              "      <td>U2OS</td>\n",
              "      <td>Training</td>\n",
              "      <td>NotUsed</td>\n",
              "      <td>105@525x56.png</td>\n",
              "      <td>/home/ubuntu/src/data/cpg0019-moshkov-deepprof...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8360445</th>\n",
              "      <td>BBBC037</td>\n",
              "      <td>41756</td>\n",
              "      <td>k08</td>\n",
              "      <td>8</td>\n",
              "      <td>995</td>\n",
              "      <td>81</td>\n",
              "      <td>../../training_images/BBBC037/41756/k08/8/105@...</td>\n",
              "      <td>EMPTY</td>\n",
              "      <td>ORF</td>\n",
              "      <td>Control</td>\n",
              "      <td>U2OS</td>\n",
              "      <td>NotUsed</td>\n",
              "      <td>NotUsed</td>\n",
              "      <td>105@995x81.png</td>\n",
              "      <td>/home/ubuntu/src/data/cpg0019-moshkov-deepprof...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Collection Metadata_Plate Metadata_Well Metadata_Site  \\\n",
              "8255825    BBBC037          41754           l09             2   \n",
              "8314697    BBBC037          41755           m16             2   \n",
              "8321958    BBBC037          41755           p23             5   \n",
              "8264069    BBBC037          41754           o21             3   \n",
              "8360445    BBBC037          41756           k08             8   \n",
              "\n",
              "         Nuclei_Location_Center_X  Nuclei_Location_Center_Y  \\\n",
              "8255825                       776                       538   \n",
              "8314697                       522                       724   \n",
              "8321958                       953                       186   \n",
              "8264069                       525                        56   \n",
              "8360445                       995                        81   \n",
              "\n",
              "                                                Image_Name Treatment  \\\n",
              "8255825  ../../training_images/BBBC037/41754/l09/2/105@...     EMPTY   \n",
              "8314697  ../../training_images/BBBC037/41755/m16/2/105@...     EMPTY   \n",
              "8321958  ../../training_images/BBBC037/41755/p23/5/105@...     EMPTY   \n",
              "8264069  ../../training_images/BBBC037/41754/o21/3/105@...     EMPTY   \n",
              "8360445  ../../training_images/BBBC037/41756/k08/8/105@...     EMPTY   \n",
              "\n",
              "        Treatment_Type  Control Cell_line LeaveReplicatesOut LeaveCellsOut  \\\n",
              "8255825            ORF  Control      U2OS           Training       NotUsed   \n",
              "8314697            ORF  Control      U2OS           Training       NotUsed   \n",
              "8321958            ORF  Control      U2OS           Training       NotUsed   \n",
              "8264069            ORF  Control      U2OS           Training       NotUsed   \n",
              "8360445            ORF  Control      U2OS            NotUsed       NotUsed   \n",
              "\n",
              "                  PathId                                               Path  \\\n",
              "8255825  105@776x538.png  /home/ubuntu/src/data/cpg0019-moshkov-deepprof...   \n",
              "8314697  105@522x724.png  /home/ubuntu/src/data/cpg0019-moshkov-deepprof...   \n",
              "8321958  105@953x186.png  /home/ubuntu/src/data/cpg0019-moshkov-deepprof...   \n",
              "8264069   105@525x56.png  /home/ubuntu/src/data/cpg0019-moshkov-deepprof...   \n",
              "8360445   105@995x81.png  /home/ubuntu/src/data/cpg0019-moshkov-deepprof...   \n",
              "\n",
              "         Labels  \n",
              "8255825       7  \n",
              "8314697       7  \n",
              "8321958       7  \n",
              "8264069       7  \n",
              "8360445       7  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c037_27k_dataset['Path'] = c037_27k_dataset.apply(lambda x: add_image_paths(x), axis=1)\n",
        "c037_27k_dataset['Labels'] = c037_27k_dataset[\"Treatment\"].replace(\n",
        "          to_replace=class_labels_to_int)\n",
        "print(c037_27k_dataset.shape)\n",
        "c037_27k_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-vsdbxR7Hs5K"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(27448, 16)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c037_27k_dataset_clean = DataFetcher.clean_data(c037_27k_dataset)\n",
        "c037_27k_dataset_clean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M7ZeNAX0UQQV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved dev data with shape (5490, 17) to /home/ubuntu/src/cache/dev\n",
            "Saved train data with shape (21958, 17) to /home/ubuntu/src/cache/train\n"
          ]
        }
      ],
      "source": [
        "DataFetcher.create_train_dev_split(c037_27k_dataset_clean, CACHE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bsmB1TG4UNsF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added channels to /home/ubuntu/src/cache/train/data.parquet, shape is: (109790, 17)\n",
            "Added channels to /home/ubuntu/src/cache/dev/data.parquet, shape is: (27450, 17)\n"
          ]
        }
      ],
      "source": [
        "# Add channel information to each train and dev dataset\n",
        "\n",
        "for dataset_t in [\"train\", \"dev\"]:\n",
        "  dataset_file = CACHE_DIR + f\"/{dataset_t}/data.parquet\"\n",
        "  dataset_df = pd.read_parquet(dataset_file, engine=\"pyarrow\")\n",
        "  dataset_df = DataFetcher.repeat_channels(dataset_df)\n",
        "  dataset_df = dataset_df.drop(columns=['__index_level_0__'])\n",
        "  dataset_df.to_parquet(dataset_file, engine=\"pyarrow\")\n",
        "  print(f\"Added channels to {dataset_file}, shape is: {dataset_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riwos9s8Frbj"
      },
      "source": [
        "### Create a custom pytorch Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96QFK1BfF3Ro"
      },
      "source": [
        "### Check to see if custom Dataset is working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfvWag_GogEB"
      },
      "outputs": [],
      "source": [
        "# training_data = MaxVitDataset(CFG, \"train\")\n",
        "# dev_data = MaxVitDataset(CFG, \"dev\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLuAKfJRKrGr"
      },
      "outputs": [],
      "source": [
        "# print(\"Training and dev data sizes\")\n",
        "# print(len(training_data))\n",
        "# print(len(dev_data))\n",
        "# print(\"Training and dev data at idx\")\n",
        "# print(training_data[0][0].shape)\n",
        "# print(training_data[0][0].dtype)\n",
        "# print(training_data[0][0])\n",
        "# print(training_data[0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x34kalKxKg5L"
      },
      "source": [
        "### Inspect a subset of images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytl8lilzBql2"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List\n",
        "\n",
        "# 1. Take in a Dataset as well as a list of class names\n",
        "def display_random_images(dataset: torch.utils.data.dataset.Dataset,\n",
        "                          classes: List[str] = None,\n",
        "                          n: int = 10,\n",
        "                          display_shape: bool = True,\n",
        "                          seed: int = None):\n",
        "    \n",
        "    # 2. Adjust display if n too high\n",
        "    if n > 10:\n",
        "        n = 10\n",
        "        display_shape = False\n",
        "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
        "    \n",
        "    # 3. Set random seed\n",
        "    if seed:\n",
        "        random.seed(seed)\n",
        "\n",
        "    # 4. Get random sample indexes\n",
        "    random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
        "\n",
        "    # 5. Setup plot\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    # 6. Loop through samples and display random samples \n",
        "    for i, targ_sample in enumerate(random_samples_idx):\n",
        "        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
        "\n",
        "        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
        "        targ_image_adjust = targ_image.permute(1, 2, 0)\n",
        "\n",
        "        # Plot adjusted samples\n",
        "        plt.subplot(1, n, i+1)\n",
        "        plt.imshow(targ_image_adjust)\n",
        "        plt.axis(\"off\")\n",
        "        title = f\"class: {targ_label}\"\n",
        "        if display_shape:\n",
        "            title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n",
        "        plt.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUBCsvD0DgSj"
      },
      "outputs": [],
      "source": [
        "# display_random_images(training_data, n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7Ve1GWlZ1dj"
      },
      "source": [
        "### Fine Tuning With Timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['maxvit_base_tf_224.in1k',\n",
              " 'maxvit_base_tf_384.in1k',\n",
              " 'maxvit_base_tf_384.in21k_ft_in1k',\n",
              " 'maxvit_base_tf_512.in1k',\n",
              " 'maxvit_base_tf_512.in21k_ft_in1k',\n",
              " 'maxvit_large_tf_224.in1k',\n",
              " 'maxvit_large_tf_384.in1k',\n",
              " 'maxvit_large_tf_384.in21k_ft_in1k',\n",
              " 'maxvit_large_tf_512.in1k',\n",
              " 'maxvit_large_tf_512.in21k_ft_in1k',\n",
              " 'maxvit_nano_rw_256.sw_in1k',\n",
              " 'maxvit_rmlp_base_rw_224.sw_in12k',\n",
              " 'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k',\n",
              " 'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k',\n",
              " 'maxvit_rmlp_nano_rw_256.sw_in1k',\n",
              " 'maxvit_rmlp_pico_rw_256.sw_in1k',\n",
              " 'maxvit_rmlp_small_rw_224.sw_in1k',\n",
              " 'maxvit_rmlp_tiny_rw_256.sw_in1k',\n",
              " 'maxvit_small_tf_224.in1k',\n",
              " 'maxvit_small_tf_384.in1k',\n",
              " 'maxvit_small_tf_512.in1k',\n",
              " 'maxvit_tiny_rw_224.sw_in1k',\n",
              " 'maxvit_tiny_tf_224.in1k',\n",
              " 'maxvit_tiny_tf_384.in1k',\n",
              " 'maxvit_tiny_tf_512.in1k',\n",
              " 'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
              " 'maxvit_xlarge_tf_512.in21k_ft_in1k']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "timm.list_models(\"**maxvit**\", pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vAD6_OayJhsT"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "  data_dir = CACHE_DIR\n",
        "  debug = False\n",
        "  n_gpu = 1\n",
        "  # device = \"cpu\" # ['cpu', 'mps']\n",
        "  img_size = 224\n",
        "  ### total # of classes in this dataset\n",
        "  num_classes = len(class_labels_to_int)\n",
        "  ### model\n",
        "  model_name = 'maxvit_large_tf_224'\n",
        "  checkpoint = 'maxvit_large_tf_224'\n",
        "  pretrained=True\n",
        "  batch_size = 20\n",
        "  num_epochs = 30\n",
        "  num_workers = 16\n",
        "\n",
        "  ### set only one to True\n",
        "  save_best_loss = False\n",
        "  save_best_accuracy = True\n",
        "\n",
        "  optimizer = 'adamw' # [\"rmsprop\", \"adam\"]\n",
        "  learning_rate = 5e-5\n",
        "  adam_epsilon = 1e-6\n",
        "  weight_decay = 1e-8 # for adamw\n",
        "  l2_penalty = 0.01 # for RMSprop\n",
        "  rms_momentum = 0 # for RMSprop\n",
        "\n",
        "  ### learning rate scheduler (LRS)\n",
        "  scheduler = 'ReduceLROnPlateau' # []\n",
        "  # scheduler = 'CosineAnnealingLR'\n",
        "  plateau_factor = 0.5\n",
        "  plateau_patience = 3\n",
        "  cosine_T_max = 4\n",
        "  cosine_eta_min = 1e-8\n",
        "  verbose = True\n",
        "\n",
        "  ### train and validation DataLoaders\n",
        "  shuffle = True\n",
        "\n",
        "  random_seed = 42\n",
        "\n",
        "  output_dir = MODEL_OUTPUT_DIR + '/' + str(date.today())\n",
        "  checkpoint_last = output_dir + '/' + model_name + '/checkpoint-last'\n",
        "  checkpoint_best = output_dir + '/' + model_name + '/checkpoint-best'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OCoVXjIL-lO7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcankoc\u001b[0m (\u001b[33mcellvit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/tmp/tmptrzq4pkk/wandb/run-20230325_230030-e52b3b24a3b34ba49a2bacb981edbc70</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cellvit/Can-c037_27k--cellvit-maxvit_large_tf_224/runs/e52b3b24a3b34ba49a2bacb981edbc70' target=\"_blank\">e52b3b24a3b34ba49a2bacb981edbc70</a></strong> to <a href='https://wandb.ai/cellvit/Can-c037_27k--cellvit-maxvit_large_tf_224' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cellvit/Can-c037_27k--cellvit-maxvit_large_tf_224' target=\"_blank\">https://wandb.ai/cellvit/Can-c037_27k--cellvit-maxvit_large_tf_224</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cellvit/Can-c037_27k--cellvit-maxvit_large_tf_224/runs/e52b3b24a3b34ba49a2bacb981edbc70' target=\"_blank\">https://wandb.ai/cellvit/Can-c037_27k--cellvit-maxvit_large_tf_224/runs/e52b3b24a3b34ba49a2bacb981edbc70</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "os.environ['WANDB_API_KEY']='808606b1ec54e09c37c9c19ea6bb8d5b8a679987'\n",
        "\n",
        "class WandBLogger(object):\n",
        "    def __init__(self, variant, project, prefix=''):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        variant: dictionary of hyperparameters\n",
        "        project: name of project\n",
        "      \"\"\"\n",
        "      log_dir = tempfile.mkdtemp()\n",
        "      if prefix != '':\n",
        "          project = '{}--{}'.format(prefix, project)\n",
        "\n",
        "      wandb.init(\n",
        "          config=variant,\n",
        "          project=project,\n",
        "          dir=log_dir,\n",
        "          id=uuid.uuid4().hex,\n",
        "      )\n",
        "\n",
        "    def log(self, *args, **kwargs):\n",
        "      wandb.log(*args, **kwargs)\n",
        "\n",
        "wblogger = WandBLogger(\n",
        "    variant={\n",
        "      'initial_learning_rate': CFG.learning_rate,\n",
        "      'adam_epsilon': CFG.adam_epsilon,\n",
        "      'num_epochs': CFG.num_epochs,\n",
        "      'batch_size': CFG.batch_size\n",
        "    },\n",
        "    project=f'cellvit-{CFG.model_name}',\n",
        "    prefix='Can-c037_27k'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0Y1PVZk6X2dO"
      },
      "outputs": [],
      "source": [
        "from maxvit_dataset import MaxVitDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8v-t430TD61V"
      },
      "outputs": [],
      "source": [
        "class MaxVitClassifier(nn.Module):\n",
        "    def __init__(self, cfg, checkpoint=None):\n",
        "        super().__init__()\n",
        "        self.model_name = cfg.model_name\n",
        "        self.model = timm.create_model(cfg.model_name, \n",
        "                                       pretrained=cfg.pretrained, \n",
        "                                       num_classes=cfg.num_classes)\n",
        "        # n_features = self.model.head.in_features\n",
        "        # self.model.head = nn.Linear(n_features, num_classes)\n",
        "        # self.model.fc = nn.Linear(n_features, num_classes)\n",
        "        if checkpoint:\n",
        "          self.model.load_state_dict(torch.load(checkpoint), strict=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "    \n",
        "    def freeze(self):\n",
        "        # To freeze the residual layers\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in self.model.head.parameters():\n",
        "            param.requires_grad = True\n",
        "    \n",
        "    def unfreeze(self):\n",
        "        # Unfreeze all layers\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Sx2gF_qeEaga"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sizes: {'train': 109790, 'dev': 27450}\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'dev': transforms.Compose([\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {x: MaxVitDataset(CFG, split=x,\n",
        "                                   transform=data_transforms[x])\n",
        "                  for x in ['train', 'dev']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
        "                                              batch_size=CFG.batch_size,\n",
        "                                              num_workers=CFG.num_workers,\n",
        "                                              shuffle=True)\n",
        "              for x in ['train', 'dev']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'dev']}\n",
        "# class_names = image_datasets['train'].classes\n",
        "print(f\"Dataset sizes: {dataset_sizes}\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else CFG.device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fUUKdWLEEVjf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def set_seed(cfg):\n",
        "    random.seed(cfg.random_seed)\n",
        "    np.random.seed(cfg.random_seed)\n",
        "    torch.manual_seed(cfg.random_seed)\n",
        "    if cfg.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(cfg.random_seed)\n",
        "\n",
        "def train_model(cfg, model, dataloaders, criterion, optimizer):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "\n",
        "    last_checkpoint_path = CFG.checkpoint_last\n",
        "    last_scheduler_path = os.path.join(last_checkpoint_path, 'scheduler.pt')\n",
        "    last_optimizer_path = os.path.join(last_checkpoint_path, 'optimizer.pt')\n",
        "    best_checkpoint_path = CFG.checkpoint_best\n",
        "    best_scheduler_path = os.path.join(best_checkpoint_path, 'scheduler.pt')\n",
        "    best_optimizer_path = os.path.join(best_checkpoint_path, 'optimizer.pt')\n",
        "\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, cfg.num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        wblogdict = {}\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'dev']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            wblogdict[f'{phase}/loss'] = np.round(epoch_loss, 4)\n",
        "            wblogdict[f'{phase}/acc'] = np.round(epoch_acc.cpu(), 4)\n",
        "\n",
        "            if phase == \"train\":\n",
        "              wblogdict['train/learning_rate'] = CFG.learning_rate\n",
        "\n",
        "            if not os.path.exists(last_checkpoint_path):\n",
        "                os.makedirs(last_checkpoint_path)\n",
        "            \n",
        "            # torch.save(model.state_dict(), last_checkpoint_path + f\"/MaxVitModel_ep{epoch_acc}.pth\")\n",
        "            # torch.save(optimizer.state_dict(), last_optimizer_path)\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'dev' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            \n",
        "                if not os.path.exists(best_checkpoint_path):\n",
        "                    os.makedirs(best_checkpoint_path)\n",
        "\n",
        "                torch.save(model.state_dict(), best_checkpoint_path + f\"/MaxVitModel_ep{best_acc}.pth\")\n",
        "                torch.save(optimizer.state_dict(), best_optimizer_path)\n",
        "  \n",
        "            if phase == 'dev':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "                # scheduler.step(epoch_loss)\n",
        "\n",
        "        wblogger.log(wblogdict)\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mu4FdLMfHKCL"
      },
      "outputs": [],
      "source": [
        "# model_ft = models.resnet18(pretrained=True)\n",
        "# num_ftrs = model_ft.fc.in_features\n",
        "# # Here the size of each output sample is set to 2.\n",
        "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "# model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "set_seed(CFG)\n",
        "\n",
        "checkpoint = CELL_PAINTING_DIR + '/MaxVitModel_ep0.669.pth'\n",
        "model_ft = MaxVitClassifier(CFG, checkpoint=checkpoint)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "params_to_update = model_ft.parameters()\n",
        "# print(\"Params to learn:\")\n",
        "\n",
        "# for name,param in model_ft.named_parameters():\n",
        "#     if param.requires_grad == True:\n",
        "#             print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=5e-5, momentum=0.9)\n",
        "\n",
        "optimizer_ft = AdamW(model_ft.parameters(), lr=CFG.learning_rate, eps=CFG.adam_epsilon, weight_decay=CFG.weight_decay)\n",
        "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor=CFG.plateau_factor, patience=CFG.plateau_patience)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-HtsnLwtLlKE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/29\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|█▌                                                                                                        | 104/6862 [01:09<1:14:59,  1.50it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_ft, hist \u001b[39m=\u001b[39m train_model(CFG, model_ft, dataloaders, criterion, optimizer_ft)\n",
            "Cell \u001b[0;32mIn[9], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(cfg, model, dataloaders, criterion, optimizer)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     60\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 61\u001b[0m         optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     63\u001b[0m \u001b[39m# statistics\u001b[39;00m\n\u001b[1;32m     64\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    163\u001b[0m           grads,\n\u001b[1;32m    164\u001b[0m           exp_avgs,\n\u001b[1;32m    165\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    167\u001b[0m           state_steps,\n\u001b[1;32m    168\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    169\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    170\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    171\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m func(params,\n\u001b[1;32m    220\u001b[0m      grads,\n\u001b[1;32m    221\u001b[0m      exp_avgs,\n\u001b[1;32m    222\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    224\u001b[0m      state_steps,\n\u001b[1;32m    225\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    226\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    227\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    228\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    229\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    230\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    232\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
            "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/optim/adamw.py:316\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    314\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    318\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Waiting for W&B process to finish... (success).\n",
            "wandb: - 0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\r"
          ]
        }
      ],
      "source": [
        "model_ft, hist = train_model(CFG, model_ft, dataloaders, criterion, optimizer_ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn2UMID6gR5G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
