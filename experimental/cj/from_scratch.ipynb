{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from datetime import date\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import wandb\n",
    "import uuid\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.cuda.amp import GradScaler\n",
    "# from torchmetrics.classification import MulticlassAccuracy, F1\n",
    "\n",
    "import timm\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=0\n",
    "SEED=1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) available: 1\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Print num GPUs available\n",
    "print(f\"GPU(s) available: {torch.cuda.device_count()}\") \n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  ARCH = 'maxvit_large_tf_224'\n",
    "  START_EPOCH = 0\n",
    "  EPOCHS = 20\n",
    "  LR = 0.1\n",
    "  MOMENTUM = 0.9\n",
    "  WEIGHT_DECAY = 1e-4\n",
    "  ADAM_EPSILON = 1e-6\n",
    "  PRINT_FREQ = 200\n",
    "  TRAIN_BATCH = 22\n",
    "  VAL_BATCH = 22\n",
    "  WORKERS = 4\n",
    "  DATADIR = \"/data/home/ec2-user/broad/training_images/BBBC037/\"\n",
    "  TRAINDIR = DATADIR+\"train\"\n",
    "  VALDIR = DATADIR+\"val\"\n",
    "  TESTDIR = DATADIR+\"test\"\n",
    "\n",
    "  PRETRAINED = False\n",
    "  IMAGE_SIZE = 224\n",
    "  IN_CHANS = 5\n",
    "  NUM_CLASSES = 46\n",
    "\n",
    "  LEARNING_RATE = 0.01\n",
    "  ADAM_EPSILON = 1e-6\n",
    "  # WEIGHT_DECAY = 0.01 # for adamw\n",
    "  L2_PENALTY = 0.01 # for RMSprop\n",
    "  RMS_MOMENTEM = 0 # for RMSprop\n",
    "\n",
    "  ### learning rate scheduler (LRS)\n",
    "  # scheduler = 'ReduceLROnPlateau' # []\n",
    "  # scheduler = 'CosineAnnealingLR'\n",
    "  PLATEAU_FACTOR = 0.5\n",
    "  PLATEAU_PATIENCE = 3\n",
    "\n",
    "  RANDOM_SEED = 42\n",
    "\n",
    "  OUTPUT_DIR = '/home/ubuntu' + '/saved_models/' + str(date.today())\n",
    "  CHECKPOINT_LAST = OUTPUT_DIR + '/' + ARCH + '/checkpoint-last'\n",
    "  CHECKPOINT_BEST = OUTPUT_DIR + '/' + ARCH + '/checkpoint-best'\n",
    "\n",
    "  WANDB_NOTEBOOK_NAME = str(date.today()) + '_' + ARCH + '_cjdonahoe'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcjdonahoe\u001b[0m (\u001b[33mcellvit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmp60z0wp7b/wandb/run-20230406_204740-2a92505e72c04aac99b72bf7538b4f06</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cellvit/cjdonahoe--cellvit/runs/2a92505e72c04aac99b72bf7538b4f06' target=\"_blank\">2a92505e72c04aac99b72bf7538b4f06</a></strong> to <a href='https://wandb.ai/cellvit/cjdonahoe--cellvit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cellvit/cjdonahoe--cellvit' target=\"_blank\">https://wandb.ai/cellvit/cjdonahoe--cellvit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cellvit/cjdonahoe--cellvit/runs/2a92505e72c04aac99b72bf7538b4f06' target=\"_blank\">https://wandb.ai/cellvit/cjdonahoe--cellvit/runs/2a92505e72c04aac99b72bf7538b4f06</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['WANDB_API_KEY']='0f110ed76bb0d63c6552597f89bfc99bf2469e43'\n",
    "\n",
    "class WandBLogger(object):\n",
    "    def __init__(self, variant, project, prefix=''):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "        variant: dictionary of hyperparameters\n",
    "        project: name of project\n",
    "      \"\"\"\n",
    "      log_dir = tempfile.mkdtemp()\n",
    "      if prefix != '':\n",
    "          project = '{}--{}'.format(prefix, project)\n",
    "\n",
    "      wandb.init(\n",
    "          config=variant,\n",
    "          project=project,\n",
    "          dir=log_dir,\n",
    "          id=uuid.uuid4().hex,\n",
    "      )\n",
    "\n",
    "    def log(self, *args, **kwargs):\n",
    "      wandb.log(*args, **kwargs)\n",
    "\n",
    "wblogger = WandBLogger(\n",
    "    variant={\n",
    "      'initial_learning_rate': CFG.LR,\n",
    "      'adam_epsilon': CFG.ADAM_EPSILON,\n",
    "      'num_epochs': CFG.EPOCHS,\n",
    "      'batch_size': CFG.TRAIN_BATCH,\n",
    "      'weight_decay': CFG.WEIGHT_DECAY,\n",
    "      'architecture': CFG.ARCH,\n",
    "      'pretrained': CFG.PRETRAINED,\n",
    "    },\n",
    "    project=f'cellvit',\n",
    "    prefix='cjdonahoe'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(dataset):\n",
    "    ''' Get class weights for a dataset\n",
    "    Args:\n",
    "        dataset: torch.utils.data.Dataset\n",
    "    Returns:\n",
    "        class_weights: torch.FloatTensor\n",
    "    '''\n",
    "    \n",
    "    class_counts = Counter(dataset.targets)\n",
    "    n_classes = len(class_counts.keys())\n",
    "    total_count = len(dataset.targets)\n",
    "    class_weights = list({class_id: total_count/(n_classes * class_counts) for class_id, class_counts in class_counts.items()}.values())\n",
    "    class_weights = torch.FloatTensor(class_weights).cuda()\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitTensorToFiveChannels(object):\n",
    "    \"\"\"Convert images in Pytorch Dataset to Tensors with one channel\n",
    "    for each discrete fluerecent image in a Cell Painting sample.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # select the first channel since the image is grayscale\n",
    "        img = img[0,:,:]\n",
    "        # split the image into the 6 channels and remove the last channel\n",
    "        img = torch.tensor_split(img,6,dim=1)[:-1]\n",
    "        # concatenate the 5 channels into a single tensor\n",
    "        img = torch.stack(img, dim=0)\n",
    "        return img\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxVitClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxVitClassifier(nn.Module):\n",
    "    def __init__(self, checkpoint=None):\n",
    "        super().__init__()\n",
    "        self.model_name = CFG.ARCH\n",
    "        self.model = timm.create_model(\n",
    "            CFG.ARCH,\n",
    "            in_chans=CFG.IN_CHANS,\n",
    "            pretrained=CFG.PRETRAINED, \n",
    "            num_classes=CFG.NUM_CLASSES)\n",
    "        # n_features = self.model.head.in_features\n",
    "        # self.model.head = nn.Linear(n_features, num_classes)\n",
    "        # self.model.fc = nn.Linear(n_features, num_classes)\n",
    "        # if checkpoint:\n",
    "        #   self.model.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def freeze(self):\n",
    "        # To freeze the residual layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def unfreeze(self):\n",
    "        # Unfreeze all layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # Grad Scaler\n",
    "    scaler = GradScaler()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    end = time.time()\n",
    "    for images, target in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # move data to GPU\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            _, preds = torch.max(output, 1)\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "        \n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        # Backward passes under autocast are not recommended.\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # if i % CFG.PRINT_FREQ == 0:\n",
    "        #     progress.display(i)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    \n",
    "    # print('{} Loss: {:.4f} Acc: {:.4f}'.format(\"train\", epoch_loss, epoch_acc))\n",
    "    wandb.log({\"train/loss\": losses.avg, 'train/acc1': top1.avg, 'train/acc5': top5.avg})\n",
    "\n",
    "    wblogdict[f'{\"train\"}/loss'] = np.round(epoch_loss, 4)\n",
    "    wblogdict[f'{\"train\"}/acc'] = np.round(epoch_acc.cpu(), 3)\n",
    "    wblogdict['train/learning_rate'] = CFG.LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Validation: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for images, target in enumerate(val_loader):\n",
    "            # move data to GPU\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            _, preds = torch.max(output, 1)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # if i % CFG.PRINT_FREQ == 0:\n",
    "            #     progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "    \n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\"Validation\", epoch_loss, epoch_acc))\n",
    "\n",
    "    wblogdict[f'{\"val\"}/loss'] = np.round(epoch_loss, 4)\n",
    "    wblogdict[f'{\"val\"}/acc'] = np.round(epoch_acc.cpu(), 3)\n",
    "\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    wandb.log({\"val/loss\": losses.avg, 'val/acc1': top1.avg, 'val/acc5': top5.avg})\n",
    "    # wandb.log({'lr': scheduler.get_last_lr()[0]})\n",
    "    return top1.avg, top5.avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations, Datasets, & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Resize((CFG.IMAGE_SIZE, CFG.IMAGE_SIZE*6)),\n",
    "    transforms.ToTensor(),\n",
    "    SplitTensorToFiveChannels(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5, 0.5, 0.5], std=[0.225, 0.225, 0.225, 0.225, 0.225]),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMAGE_SIZE, CFG.IMAGE_SIZE*6)),\n",
    "    transforms.ToTensor(),\n",
    "    SplitTensorToFiveChannels(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5, 0.5, 0.5], std=[0.225, 0.225, 0.225, 0.225, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    CFG.TRAINDIR, transform=transform_train)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    CFG.VALDIR, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weight = get_class_weights(train_dataset)\n",
    "# # sampler = data.WeightedRandomSampler(sample_weight, len(train_dataset))\n",
    "# sampler = data.WeightedRandomSampler(sample_weight, 1000*CFG.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=CFG.TRAIN_BATCH, shuffle=True,\n",
    "        num_workers=CFG.WORKERS, pin_memory=True, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=CFG.VAL_BATCH, shuffle=True,\n",
    "        num_workers=CFG.WORKERS, pin_memory=True, sampler=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(MaxVitClassifier(CFG))\n",
    "model = model.cuda(GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "# get the class weights from the validation set weight=get_class_weights(train_dataset)\n",
    "criterion = nn.CrossEntropyLoss(weight=get_class_weights(train_dataset)).cuda(GPU)\n",
    "\n",
    "# optimizer = torch.optim.Adam(\n",
    "#   model.parameters(), \n",
    "#   lr=CFG.LR, \n",
    "#   # momentum=MOMENTUM, \n",
    "#   weight_decay=CFG.WEIGHT_DECAY\n",
    "#   )\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.EPOCHS)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.LEARNING_RATE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG.LEARNING_RATE)\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     model.parameters(),\n",
    "#     lr=CFG.LEARNING_RATE,\n",
    "#     momentum=CFG.MOMENTUM,\n",
    "#     weight_decay=CFG.WEIGHT_DECAY\n",
    "#     )\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max = CFG.EPOCHS, eta_min = 1e-4)\n",
    "    # T_0 = 8,# Number of iterations for the first restart\n",
    "    # T_mult = 1, # A factor increases TiTiâ€‹ after a restart\n",
    "    # eta_min = 1e-4) # Minimum learning rate\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# move data to GPU\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m GPU \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39;49mcuda(GPU, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m     30\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mcuda(GPU, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in tqdm(range(CFG.START_EPOCH, CFG.EPOCHS)):\n",
    "    print('Epoch {}/{}'.format(epoch, CFG.EPOCHS - 1))\n",
    "    print('-' * 20)\n",
    "    \n",
    "    wblogdict = {}\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1, acc5 = validate(val_loader, model, criterion)\n",
    "\n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': CFG.ARCH,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'acc5': acc5,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "\n",
    "    wblogger.log(wblogdict, step=epoch)\n",
    "\n",
    "    scheduler.step()\n",
    "    print('lr: ' + str(scheduler.get_last_lr()[0]))\n",
    "    wandb.log({'lr': scheduler.get_last_lr()[0]})\n",
    "    \n",
    "    # scheduler.step(epoch_loss)\n",
    "    # print('lr: ' + str(scheduler.get_last_lr()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-painting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
