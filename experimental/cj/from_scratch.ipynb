{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from datetime import date\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import wandb\n",
    "import uuid\n",
    "import tempfile\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.cuda.amp import GradScaler\n",
    "# from torchmetrics.classification import MulticlassAccuracy, F1\n",
    "\n",
    "import timm\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=0\n",
    "SEED=1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU(s) available: 1\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Print num GPUs available\n",
    "print(f\"GPU(s) available: {torch.cuda.device_count()}\") \n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  ARCH = 'maxvit_small_tf_224'\n",
    "  START_EPOCH = 0\n",
    "  EPOCHS = 30\n",
    "\n",
    "  TRAIN_BATCH = 64\n",
    "  VAL_BATCH = 64\n",
    "  \n",
    "  PRINT_FREQ = 200\n",
    "  \n",
    "  WORKERS = 4\n",
    "\n",
    "  DATADIR = \"/data/home/ec2-user/broad/training_images/BBBC037/\"\n",
    "  TRAINDIR = DATADIR+\"train\"\n",
    "  VALDIR = DATADIR+\"val\"\n",
    "  TESTDIR = DATADIR+\"test\"\n",
    "\n",
    "  PRETRAINED = False\n",
    "  IMAGE_SIZE = 224\n",
    "  IN_CHANS = 5\n",
    "  NUM_CLASSES = 46\n",
    "\n",
    "  ### optimizer\n",
    "  LR = 0.01\n",
    "  MOMENTUM = 0.9\n",
    "  ADAM_EPSILON = 1e-6\n",
    "  WEIGHT_DECAY = 1e-8 # for AdamW\n",
    "\n",
    "  RANDOM_SEED = 42\n",
    "\n",
    "  OUTPUT_DIR = '/home/ubuntu' + '/saved_models/' + str(date.today())\n",
    "  CHECKPOINT_LAST = OUTPUT_DIR + '/' + ARCH + '/checkpoint-last'\n",
    "  CHECKPOINT_BEST = OUTPUT_DIR + '/' + ARCH + '/checkpoint-best'\n",
    "\n",
    "  WANDB_NOTEBOOK_NAME = str(date.today()) + '_' + ARCH + '_cjdonahoe'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcjdonahoe\u001b[0m (\u001b[33mcellvit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/tmpm5vwvhcn/wandb/run-20230406_215833-494a7fad5f8841df9faa198473f033eb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cellvit/cjdonahoe--cellvit/runs/494a7fad5f8841df9faa198473f033eb' target=\"_blank\">494a7fad5f8841df9faa198473f033eb</a></strong> to <a href='https://wandb.ai/cellvit/cjdonahoe--cellvit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cellvit/cjdonahoe--cellvit' target=\"_blank\">https://wandb.ai/cellvit/cjdonahoe--cellvit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cellvit/cjdonahoe--cellvit/runs/494a7fad5f8841df9faa198473f033eb' target=\"_blank\">https://wandb.ai/cellvit/cjdonahoe--cellvit/runs/494a7fad5f8841df9faa198473f033eb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['WANDB_API_KEY']='0f110ed76bb0d63c6552597f89bfc99bf2469e43'\n",
    "\n",
    "class WandBLogger(object):\n",
    "    def __init__(self, variant, project, prefix=''):\n",
    "      \"\"\"\n",
    "      Args:\n",
    "        variant: dictionary of hyperparameters\n",
    "        project: name of project\n",
    "      \"\"\"\n",
    "      log_dir = tempfile.mkdtemp()\n",
    "      if prefix != '':\n",
    "          project = '{}--{}'.format(prefix, project)\n",
    "\n",
    "      wandb.init(\n",
    "          config=variant,\n",
    "          project=project,\n",
    "          dir=log_dir,\n",
    "          id=uuid.uuid4().hex,\n",
    "      )\n",
    "\n",
    "    def log(self, *args, **kwargs):\n",
    "      wandb.log(*args, **kwargs)\n",
    "\n",
    "wblogger = WandBLogger(\n",
    "    variant={\n",
    "      'initial_learning_rate': CFG.LR,\n",
    "      'adam_epsilon': CFG.ADAM_EPSILON,\n",
    "      'num_epochs': CFG.EPOCHS,\n",
    "      'batch_size': CFG.TRAIN_BATCH,\n",
    "      'weight_decay': CFG.WEIGHT_DECAY,\n",
    "      'architecture': CFG.ARCH,\n",
    "      'pretrained': CFG.PRETRAINED,\n",
    "    },\n",
    "    project=f'cellvit',\n",
    "    prefix='cjdonahoe'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(dataset):\n",
    "    ''' Get class weights for a dataset\n",
    "    Args:\n",
    "        dataset: torch.utils.data.Dataset\n",
    "    Returns:\n",
    "        class_weights: torch.FloatTensor\n",
    "    '''\n",
    "    \n",
    "    class_counts = Counter(dataset.targets)\n",
    "    n_classes = len(class_counts.keys())\n",
    "    total_count = len(dataset.targets)\n",
    "    class_weights = list({class_id: total_count/(n_classes * class_counts) for class_id, class_counts in class_counts.items()}.values())\n",
    "    class_weights = torch.FloatTensor(class_weights).cuda()\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitTensorToFiveChannels(object):\n",
    "    \"\"\"Convert images in Pytorch Dataset to Tensors with one channel\n",
    "    for each discrete fluerecent image in a Cell Painting sample.\"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # select the first channel since the image is grayscale\n",
    "        img = img[0,:,:]\n",
    "        # split the image into the 6 channels and remove the last channel\n",
    "        img = torch.tensor_split(img,6,dim=1)[:-1]\n",
    "        # concatenate the 5 channels into a single tensor\n",
    "        img = torch.stack(img, dim=0)\n",
    "        return img\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxVitClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxVitClassifier(nn.Module):\n",
    "    def __init__(self, checkpoint=None):\n",
    "        super().__init__()\n",
    "        self.model_name = CFG.ARCH\n",
    "        self.model = timm.create_model(\n",
    "            CFG.ARCH,\n",
    "            in_chans=CFG.IN_CHANS,\n",
    "            pretrained=CFG.PRETRAINED, \n",
    "            num_classes=CFG.NUM_CLASSES)\n",
    "        # n_features = self.model.head.in_features\n",
    "        # self.model.head = nn.Linear(n_features, num_classes)\n",
    "        # self.model.fc = nn.Linear(n_features, num_classes)\n",
    "        # if checkpoint:\n",
    "        #   self.model.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def freeze(self):\n",
    "        # To freeze the residual layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.model.head.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def unfreeze(self):\n",
    "        # Unfreeze all layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # Grad Scaler\n",
    "    scaler = GradScaler()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # move data to GPU\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            _, preds = torch.max(output, 1)\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "        \n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        # Backward passes under autocast are not recommended.\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % CFG.PRINT_FREQ == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    \n",
    "    # print('{} Loss: {:.4f} Acc: {:.4f}'.format(\"train\", epoch_loss, epoch_acc))\n",
    "    wandb.log({\"train/loss\": losses.avg, 'train/acc1': top1.avg, 'train/acc5': top5.avg})\n",
    "\n",
    "    wblogdict[f'{\"train\"}/loss'] = np.round(epoch_loss, 4)\n",
    "    wblogdict[f'{\"train\"}/acc'] = np.round(epoch_acc.cpu(), 3)\n",
    "    wblogdict['train/learning_rate'] = CFG.LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Validation: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            # move data to GPU\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            _, preds = torch.max(output, 1)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % CFG.PRINT_FREQ == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(val_loader.dataset)\n",
    "    \n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\"Validation\", epoch_loss, epoch_acc))\n",
    "\n",
    "    wblogdict[f'{\"val\"}/loss'] = np.round(epoch_loss, 4)\n",
    "    wblogdict[f'{\"val\"}/acc'] = np.round(epoch_acc.cpu(), 3)\n",
    "\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    wandb.log({\"val/loss\": losses.avg, 'val/acc1': top1.avg, 'val/acc5': top5.avg})\n",
    "    # wandb.log({'lr': scheduler.get_last_lr()[0]})\n",
    "    return top1.avg, top5.avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# def set_seed(cfg):\n",
    "#     random.seed(cfg.random_seed)\n",
    "#     np.random.seed(cfg.random_seed)\n",
    "#     torch.manual_seed(cfg.random_seed)\n",
    "#     if cfg.n_gpu > 0:\n",
    "#         torch.cuda.manual_seed_all(cfg.random_seed)\n",
    "\n",
    "# def train_model(cfg, model, dataloaders, criterion, optimizer):\n",
    "#     since = time.time()\n",
    "\n",
    "#     val_acc_history = []\n",
    "\n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "\n",
    "#     last_checkpoint_path = CFG.checkpoint_last\n",
    "#     last_scheduler_path = os.path.join(last_checkpoint_path, 'scheduler.pt')\n",
    "#     last_optimizer_path = os.path.join(last_checkpoint_path, 'optimizer.pt')\n",
    "#     best_checkpoint_path = CFG.checkpoint_best\n",
    "#     best_scheduler_path = os.path.join(best_checkpoint_path, 'scheduler.pt')\n",
    "#     best_optimizer_path = os.path.join(best_checkpoint_path, 'optimizer.pt')\n",
    "\n",
    "#     for epoch in range(cfg.num_epochs):\n",
    "#         print('Epoch {}/{}'.format(epoch, cfg.num_epochs - 1))\n",
    "#         print('-' * 10)\n",
    "\n",
    "#         wblogdict = {}\n",
    "\n",
    "#         # Each epoch has a training and validation phase\n",
    "#         for phase in ['train', 'val']:\n",
    "#             if phase == 'train':\n",
    "#                 scaler = GradScaler()\n",
    "#                 model.train()  # Set model to training mode\n",
    "#             else:\n",
    "#                 model.eval()   # Set model to evaluate mode\n",
    "\n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "\n",
    "#             # Iterate over data.\n",
    "#             for inputs, labels in tqdm(dataloaders[phase]):\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 # zero the parameter gradients\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 # forward\n",
    "#                 # track history if only in train\n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "#                     # Get model outputs and calculate loss\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "\n",
    "#                     # backward + optimize only if in training phase\n",
    "#                     if phase == 'train':\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "\n",
    "#                 # statistics\n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "#             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "#             wblogdict[f'{phase}/loss'] = np.round(epoch_loss, 4)\n",
    "#             wblogdict[f'{phase}/acc'] = np.round(epoch_acc.cpu(), 4)\n",
    "\n",
    "#             if phase == \"train\":\n",
    "#               wblogdict['train/learning_rate'] = CFG.learning_rate\n",
    "\n",
    "#             if not os.path.exists(last_checkpoint_path):\n",
    "#                 os.makedirs(last_checkpoint_path)\n",
    "            \n",
    "#             # torch.save(model.state_dict(), last_checkpoint_path + f\"/MaxVitModel_ep{epoch_acc}.pth\")\n",
    "#             # torch.save(optimizer.state_dict(), last_optimizer_path)\n",
    "\n",
    "#             # deep copy the model\n",
    "#             if phase == 'dev' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "#                 if not os.path.exists(best_checkpoint_path):\n",
    "#                     os.makedirs(best_checkpoint_path)\n",
    "\n",
    "#                 torch.save(model.state_dict(), best_checkpoint_path + f\"/MaxVitModel_ep{best_acc}.pth\")\n",
    "#                 torch.save(optimizer.state_dict(), best_optimizer_path)\n",
    "  \n",
    "#             if phase == 'dev':\n",
    "#                 val_acc_history.append(epoch_acc)\n",
    "#                 # scheduler.step(epoch_loss)\n",
    "\n",
    "#         wblogger.log(wblogdict)\n",
    "#         print()\n",
    "\n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "#     # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "#     return model, val_acc_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations, Datasets, & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Resize((CFG.IMAGE_SIZE, CFG.IMAGE_SIZE*6)),\n",
    "    transforms.ToTensor(),\n",
    "    SplitTensorToFiveChannels(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5, 0.5, 0.5], std=[0.225, 0.225, 0.225, 0.225, 0.225]),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((CFG.IMAGE_SIZE, CFG.IMAGE_SIZE*6)),\n",
    "    transforms.ToTensor(),\n",
    "    SplitTensorToFiveChannels(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5, 0.5, 0.5], std=[0.225, 0.225, 0.225, 0.225, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    CFG.TRAINDIR, transform=transform_train)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    CFG.VALDIR, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weight = get_class_weights(train_dataset)\n",
    "# # sampler = data.WeightedRandomSampler(sample_weight, len(train_dataset))\n",
    "# sampler = data.WeightedRandomSampler(sample_weight, 1000*CFG.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=CFG.TRAIN_BATCH, shuffle=True,\n",
    "        num_workers=CFG.WORKERS, pin_memory=True, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=CFG.VAL_BATCH, shuffle=False,\n",
    "        num_workers=CFG.WORKERS, pin_memory=True, sampler=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(MaxVitClassifier(CFG))\n",
    "model = model.cuda(GPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "# get the class weights from the validation set weight=get_class_weights(train_dataset)\n",
    "criterion = nn.CrossEntropyLoss(weight=get_class_weights(train_dataset)).cuda(GPU)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=CFG.LR)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.LR, eps=CFG.ADAM_EPSILON, weight_decay=CFG.WEIGHT_DECAY)\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     model.parameters(),\n",
    "#     lr=CFG.LEARNING_RATE,\n",
    "#     momentum=CFG.MOMENTUM,\n",
    "#     weight_decay=CFG.WEIGHT_DECAY\n",
    "#     )\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max = CFG.EPOCHS, eta_min = 1e-4)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4325/4325 [36:05<00:00,  2.00it/s]\n",
      "100%|██████████| 1235/1235 [06:22<00:00,  3.23it/s]\n",
      "/opt/conda/envs/torch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 1.892 Acc@5 10.147\n",
      "Validation Loss: nan Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [43:08<13:39:35, 2588.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: nan\n",
      "Epoch 1/19\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 3544/4325 [20:18<04:28,  2.91it/s]\n",
      "  5%|▌         | 1/20 [1:03:26<20:05:32, 3806.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "Cell \u001b[0;32mIn[29], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     44\u001b[0m top5\u001b[39m.\u001b[39mupdate(acc5[\u001b[39m0\u001b[39m], images\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m     46\u001b[0m \u001b[39m# Scales loss.  Calls backward() on scaled loss to create scaled gradients.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# Backward passes under autocast are not recommended.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m# Backward ops run in the same dtype autocast chose for corresponding forward ops.\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m scaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     51\u001b[0m \u001b[39m# scaler.step() first unscales the gradients of the optimizer's assigned params.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# If these gradients do not contain infs or NaNs, optimizer.step() is then called,\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m# otherwise, optimizer.step() is skipped.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m scaler\u001b[39m.\u001b[39mstep(optimizer)\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(CFG.START_EPOCH, CFG.EPOCHS):\n",
    "    print('Epoch {}/{}'.format(epoch, CFG.EPOCHS - 1))\n",
    "    print('-' * 20)\n",
    "    \n",
    "    wblogdict = {}\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1, acc5 = validate(val_loader, model, criterion)\n",
    "\n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': CFG.ARCH,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'acc5': acc5,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "\n",
    "    wblogger.log(wblogdict, step=epoch)\n",
    "\n",
    "    scheduler.step()\n",
    "    print('lr: ' + str(scheduler.get_last_lr()[0]))\n",
    "    wandb.log({'lr': scheduler.get_last_lr()[0]})\n",
    "    \n",
    "    # scheduler.step(epoch_loss)\n",
    "    # print('lr: ' + str(scheduler.get_last_lr()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell-painting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
